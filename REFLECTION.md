For my data project, I chose to pull data from the Balldontlie API, an API that provides player information about players from the NBA, and the NBA stats Kaggle dataset, a comprehensive data file containing advanced player statistics which was mounted in my environment and accessed locally. I encountered many problems working through this project. The first was when pulling from the BalldontlieAPI, I hit a rate limit from the website. Since I was working from the free tier, any calls past 20 consecutive calls were cut off, so I had to implement a hard limit rather than using the logic I originally had which essentially checked if there were fewer responses than the specified page size, which would have meant I had hit the end of their player data, but with the current restrictions on access this was impossible. However, even with these restrictions in place, I was able to pull 500 players. The next issue was the local file had a lot more entries and had multiple seasons for each player, so when they were merged there were duplicate entries. To fix this I sorted by season recency and only kept the most recent season for each play. There was plenty of cleaning that had to be done as well, as I had to combine first and last names to allow for a merge on the player's full name, as well as remove columns that contained irrelevant and/or broken data. There was also an issue with college names, as there were non-colleges in the column, but this was the result of players who didn’t go to college, so this result was unavoidable. Also I encountered an issue where all the json responses were being printed which gave a wall of text that was essentially impossible to scroll past, so I had to make a dummy IO string towrite to to avoid this. One thing that was a lot easier than expected was the actual merging, transformation, and conversion of the data frames. Pandas makes life so much easier and made the file type conversions as well as cleaning much less difficult than anticipated. One thing that was more difficult than anticipated was setting up for the merge, as the two data sources didn’t have a common key, to begin with, so I had to create one with the full name column and then also clean the Kaggle set to avoid the duplicates like I described above. Overall a utility like this could be useful for future projects as more often than not data isn’t all going to be in one spot, so having the ability to take separate sources and effectively combine them so the data is in a useable state is an essential skill to have moving forward in data science.
